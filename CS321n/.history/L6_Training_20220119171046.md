#  Trainning

## Outline

1. Training Loop
2. Main point
3. Activation Functions
4. 


## Training Loop
1. Sample a batch of data
2. Forward to calculate the loss
3. Backprop to calculate the Gradients
4. Update the parameters by the Gradients

## Main point
1. One time setup
   1. Activation Functions
   2. Processing
   3. Weight Initialization
   4. Regularization
   5. Gradient Checking
2. Training Dynamics
   1. Babysitting the learning Process
   2. Parameter Updates
   3. Hyperparameter Optimization
3. Evaluation
   1. Model Ensembels
   
## Activation Functions

###  Sigmoid $$\sigma(x) = \frac{1}{1+\exp(-x)}$$
   - Squashes(压缩) numbers to range [0,1]
   - Historically popular since they have nice Interpretation a saturating "firing rate" of a neuron
   - $\frac{d\sigma(x)}{dx} = \sigma(x)(1-\sigma(x))$

3 problems:
1. Saturating neurons ''will'' kill the gradients.
        
        When the x is too large or too low,the gradient close to zero,which means ''killing the gradient''.

2. Sigmoid output are not zero-centered.
        
        If input is always positive the Gradient of W is always all positive or all negetive(this is also why you want zero-mean data) which slow the speed of updating(the change direction is limited).

3. exp() is a bit compute expensive

### tanh(x)
- squashed numbers to range[-1,1]
- zero centered(nice!)
- ''kill'' the gradients also
  
### ReLU 
$$f(x) = \max\{ Gate, x\}$$
good:
- Does not saturate (+region)
- very computationally efficient
- Converges(收敛) much faster then sigmiod
- Actually more biologically plausible then sigmiod

Bad:
- Not zero-centered
- Dead ReLU
        
        In some area it will never activate --> never update

### Leaky ReLU
$$f(x) = max\{\beta x,x\},\beta\in(0,1)$$
- Does not saturate (+region)
- Computationally efficient
- Converges much faster then sigmiod
- NOT DIE！！

#### Ex：Parametric Rectifier(PReLU):
$$PReLU(x) = max\{\alpha x,x\},\beta\in(0,1)$$
where $\alpha$ is not a hyperparameter that specified by setting,but a parameter to learning,with more flexiblity.

### Exponential Linear Units(ELU)
$$ELU(x) = \begin{cases}
            x & {x \geq 0}\\
            \alpha(\exp(x)-1) & \text{x<0}
            \end{cases}$$
- All benefits of ReLU
- Closer to zero-mean output
- Negetive saturation regime compared with Leaky ReLU adds some robustness to noise

bad:
- Computation requires exp()(expensive)