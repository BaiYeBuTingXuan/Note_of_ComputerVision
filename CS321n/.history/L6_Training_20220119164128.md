#  Trainning

## Outline

1. Training Loop
2. Main point
3. Activation Functions


## Training Loop
1. Sample a batch of data
2. Forward to calculate the loss
3. Backprop to calculate the Gradients
4. Update the parameters by the Gradients

## Main point
1. One time setup
   1. Activation Functions
   2. Processing
   3. Weight Initialization
   4. Regularization
   5. Gradient Checking
2. Training Dynamics
   1. Babysitting the learning Process
   2. Parameter Updates
   3. Hyperparameter Optimization
3. Evaluation
   1. Model Ensembels
   
## Activation Functions

###  Sigmoid $$\sigma(x) = \frac{1}{1+\exp(-x)}$$
   - Squashes(压缩) numbers to range [0,1]
   - Historically popular since they have nice Interpretation a saturating "firing rate" of a neuron
   - $\frac{d\sigma(x)}{dx} = \sigma(x)(1-\sigma(x))$

3 problems:
    1. Saturating neurons ''will'' kill the gradients.
    2. 