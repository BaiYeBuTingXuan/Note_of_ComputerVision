# Backpropagation

## Outline

1. Backpropagation
2. 


## Backpropagation

  For those Mutilayer,complex neurual model and loss function,it is difficult to
  calculate the Analytic Gradients manually.

  Backporpagetion solves this problem by take the complex calculation into pieces as 
  set of addition and multipition.With chain rulex of gradient,compute from result to the cause.

## Sigmoid Function
    The Function is usually used:
    $$ \sigma(x) = \frac{1}{1+\exp(-x)} $$

## Optimization:Gradient Descent
  Method: Caculating the Gradients.
    - Numeric Gradients: Approximate,Slow,easy to write.
    - Analytic(Derived from the expression) Gradient: Exact,Fast,Erro-prone(容易出错)
    - $\nabla_{Weight}$ is an linear opeater,so that：
    $$\nabla_{Weight} Full\_Loss = \Sigma_i \nabla_{Weight}Loss_{i}(f(x_i,W);y_i)+ \lambda \nabla_{Weight} R(Weight)$$

## Gradient Descent Method:
  $$  Weight += - \nabla Weight * Learning\_rate $$
  in Python Code:
  
  ```python
  #Train :
  while True:
    Weight_grad = Grad(Loss_func,data,Weight)
    step = -Learning_Rate*Weight_grad
    Weight += step
  ```

  then the parameter sheet $Weight$ go close to the Steady Point step by step.
    
  Q:Will it go to a local min-value but not a global min value？？

  Ans：Yes,so that we search for more path like convex optimization,Adam,momentum-motivation and so on.

## Mini-Batch Idea
  To improve the speed of the learning or precisely the update rate of the $Weight$ for a millions of data base, we can pack the train data as a Batch randomly,like:

  ```python
  #Train :
  while True:
    # batch the data:
    data_batch = sample_train_data(data,BATCH_SIZE)
    # Use the batch to guide the grad calulate
    Weight_grad = Grad(Loss_func,data_batch,Weight)
    step = -Learning_Rate*Weight_grad
    Weight += step
  ```

  In addition,BACTH_SIZE is often the power of 2,like 2,4,16,256.

## Featuring before Learning
  提取图像中的一些特征信息，根据这些信息进行学习，而不是根据完整的信息。因为心理学研究标明图像信息大多是无用的。在图像识别中，有用的一般是图像的有向边缘向量。

  卷积神经网络以及深度学习与特征提取没有太大的差别，前者是通过学习的方法认知特征，而不是先人工进行特征的提取。

  Featuring the information and learing with the results of it,but no the total infomation,for most of them is useless,according to some study of psycology. In Image Reconization,the egde arrows is the effective information as known.

  CNN and DL seem similar to Featuring,but the former is featuring by learning,not by people. 