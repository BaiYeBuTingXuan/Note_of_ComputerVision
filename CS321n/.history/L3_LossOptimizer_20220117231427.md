# Loss and Optimizer

## Outline

1. Loss Function
2. 
3. 


## Loss Function

    Loss Funtion is designed to measure the unaccuracy of the Classifier.

    "To quantify how bad are different mistakes"

## Loss Calculating Algorithm

General Format:$$Loss = \Sigma Loss_{i}(f(input;Weight);fact)$$

- SVM: 
$$Loss = \Sigma_{i \neq j} \max\{0, s_i - s_j + 1\}$$       

  note: $ i \neq j$ is necessary, or the Loss for a perfect answer is result in 1 not 0.

- Regularization(惩罚) Term: to pick a simpler $\mathbb{W}$
    $$Loss = \Sigma Loss_{i}(f(input;Weight);fact)+ \lambda \mathbb{R(Weight)}$$

    $\lambda$ called regularization strengh,is a hyporparameter to balance the two terms.

        Occam's Razor:"Among competing hypotheses, the simplest is the best."

Common use:
    - L2 regularization : $R(W) = \Sigma_i \Sigma_j W^{2}_{i,j}$
        ''Average the Weight!''
    - L1 regularization : $R(W) = \Sigma_i \Sigma_j |W_{i,j}|$ 
        "More zeros in Weight!"
    - Elastic net(L1+L2) : $R(W) = \Sigma_i \Sigma_j \beta|W_i,j|+W^{2}_{i,j}$ 
    - Max Norm Regularization
    - Dropout 

- SoftMax Loss:Mutinomial Logistic Regression(多元逻辑斯蒂回归模型)
    It give the scores with some unique meaning:
    
        scores = Unnomaized Log Probability of the Classes
    From Logistic Regression
   $$ P(Y = k| X = x_i) = \frac{e^{s_k}}{\Sigma_j e^{s_j}} $$ where s is the score = f(x;W)
    Then define the Loss be_like:
   $$Loss_i = - log P(Y = y_i| X = x_i)$$
   $$Loss = \Sigma_i Loss_i$$

   summary: Score -(exp)->-(normalize)->-(-log)->Loss
